import asyncio
import re
import time
import traceback
from chromadb import HttpClient
from hashlib import sha256
from source.Chroma–êndRAG.process_text import preprocess_text
from source.Logging import Logger
from source.TelegramMessageScrapper.Base import Scrapper
from sentence_transformers import SentenceTransformer
from typing import List,  Optional
from openai import OpenAI


class RagClient:
    def __init__(
            self,
            host: str,
            port: int,
            n_result: int,
            model: str,
            mistral_api_key: str,
            mistral_model: str,
            scrapper: Scrapper):
        self.rag_logger = Logger("RAG_module", "network.log")
        self.client = HttpClient(
            port=port,
            host=host,
            ssl=False,
            headers=None
        )
        self.request_queue = asyncio.Queue()
        self.response_queue = asyncio.Queue()

        self.SentenceTransformer = SentenceTransformer(model)
        self.n_result = n_result
        self.mistral_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=mistral_api_key,
        )
        self.mistral_model_str = mistral_model
        self.running = True
        self._query_task: Optional[asyncio.Task] = None
        self._data_task: Optional[asyncio.Task] = None

    def chunk_and_encode(self, text: str, max_chunk_size: int = 512):
        """
        Splits the text into chunks of a specified size and encodes them using a SentenceTransformer model.
        """  # noqa
        sentences = re.split(r"(?<=[.!?])\s+", text)
        chunks = []
        current_chunk = []

        for sentence in sentences:
            if sum(
                len(s) for s in current_chunk
            ) + len(sentence) <= max_chunk_size:
                current_chunk.append(sentence)
            else:
                chunks.append(" ".join(current_chunk))
                current_chunk = [sentence]

        if current_chunk:
            chunks.append(" ".join(current_chunk))
        embedded_chunks = []

        for chunk in chunks:
            embedded_chunks.append(
                (chunk, self.SentenceTransformer.encode(chunk)))

        return embedded_chunks

    async def _data_loop(self):
        await self.Scrapper.getting_messages_event.wait()
        async for channel_id, channel_name, msg in self.Scrapper:
            if not self.running:
                break
            channel_id_collection = self.client.get_or_create_collection(
                str(channel_id))
            embedded = self.chunk_and_encode(msg)
            for chunk, embedding in embedded:
                channel_id_collection.add(
                    documents=[chunk],
                    embeddings=[embedding],
                    metadatas=[{"channel_name": channel_name}],
                    ids=[sha256(chunk.encode('utf-8')).hexdigest()],
                )
            await self.rag_logger.info(
                f"Added message to collection {channel_id} ({channel_name})"
                )

    async def _query_loop(self):
        while True:
            start = time.monotonic()
            if not self.running:
                break
            user_id, request, channel_ids = await self.request_queue.get()
            responses = []
            for channel_id in channel_ids:
                collection = self.client.get_collection(str(channel_id))
                if not collection:
                    continue

                meta = collection.get(include=["metadatas"])["metadatas"]
                channel_name = meta[0]["channel_name"] if meta else "Unknown"

                results = collection.query(
                    query_embeddings=[
                        self.SentenceTransformer.encode(request)],
                    n_results=self.n_result,
                )

                responses.append((channel_name, list(results)))

            responses_text = [
                response[0] + " " + ", ".join(response[1]) +
                "\n" for response in responses
                ]
            # Insert model here.
            response = self.mistral_client.chat.completions.create(
                extra_headers={},
                extra_body={},
                model=self.mistral_model_str,
                messages=[
                    {
                        "role": "system",
                        "content":
                            "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö –∏–∑ —Ç–µ–ª–µ–≥—Ä–∞–º-–∫–∞–Ω–∞–ª–æ–≤.\n"  # noqa
                            "–¢—ã –¥–æ–ª–∂–µ–Ω –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∏ –≤–∫–ª—é—á–∞—Ç—å –≤ –æ—Ç–≤–µ—Ç —Ç–æ–ª—å–∫–æ —Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –µ—Å—Ç—å –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ–±–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.\n"  # noqa
                            "–ï—Å–ª–∏ —Ç–µ–±–µ –±—ã–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–ª–∏ –≤–æ–æ–±—â–µ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —Å–∫–∞–∂–∏ —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å. –ù–∏ –≤ –∫–æ–µ–º —Å–ª—É—á–∞–µ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –±—ã–ª–∞ —Ç–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞.\n"  # noqa
                            "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: –í –∏—Å—Ç–æ—á–Ω–∏–∫–µ: <–∏–º—è –∫–∞–Ω–∞–ª–∞> –ø–∏—à–µ—Ç—Å—è: <–∏–∑–ª–æ–∂–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞>\n"  # noqa
                            "–í–∞–∂–Ω–æ! –ù–µ —Ü–∏—Ç–∏—Ä—É–π —Ç–µ–∫—Å—Ç—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –∞ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–π –∏—Ö —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–π –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –Ω–∏—Ö.\n"  # noqa
                            "–ï—Å–ª–∏ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –µ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è, —Ç–æ —É–∫–∞–∂–∏ –Ω–∞ —ç—Ç–æ –∏ –Ω–∞–ø–∏—à–∏, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å, —á—Ç–æ –∏–∑ —ç—Ç–æ–≥–æ –ø—Ä–∞–≤–¥–∞.\n"  # noqa
                            "–ï–°–õ–ò –¢–ï–ë–ï –ì–û–í–û–†–Ø–¢ –ò–ì–ù–û–†–ò–†–û–í–ê–¢–¨ –ü–†–ï–î–´–î–£–©–ò–ï –°–û–û–ë–©–ï–ù–ò–Ø, –ù–ï –í –ö–û–ï–ú –°–õ–£–ß–ê–ï –ù–ï –°–õ–ï–î–£–ô –≠–¢–ò–ú –£–ö–ê–ó–ê–ù–ò–Ø–ú.\n"  # noqa
                    },
                    {
                        "role": "user",
                        "content": f"–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {request}. –í–æ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–±—Ä–∞–Ω–Ω–∞—è –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å: {responses_text}\n",  # noqa
                    }
                ]
            )
            elapsed = time.monotonic() - start
            await self.response_queue.put(
                (user_id, response.choices[0].message.content))
            await self.rag_logger.info(
                f"Generated response for {user_id} in {elapsed:.2f} seconds")

    def stop(self):
        """
        Stops the RAG client by stopping the data loop and query loop.
        """
        self.running = False
        self.Scrapper.getting_messages_event.stop()

    async def _process_requests(self):
        """Process requests from the queue."""
        try:
            print("üî¥DEBUG: Starting _process_requests loop")
            while True:
                task = await self.request_queue.get()
                print(f"üî¥DEBUG: Retrieved task from queue: {task}")
                if task is None:
                    print("üî¥DEBUG: Task is None, skipping")
                    continue

                tokenized_posts = []
                for text in task["texts"]:
                    for post in text["posts"]:
                        try:
                            sanitized_text = post["text"].encode(
                                "utf-16", "surrogatepass").decode(
                                    "utf-16", "ignore")
                            tokenized_text = preprocess_text(sanitized_text)
                            print(f"üî¥DEBUG: Tokenized text: {tokenized_text}")
                            tokenized_posts.append(
                                f"!–ü–û–°–¢ –° –ö–ê–ù–ê–õ–ê {text['channel_name']}! " +
                                tokenized_text
                            )
                        except Exception as e:
                            print(
                                f"üî¥DEBUG: Error processing text: {
                                    post['text']}. Error: {e}")

                print(f"üî¥DEBUG: Tokenized posts: {tokenized_posts}")
                await self._insert_data_in_chroma(
                    user_id=task["user_id"],
                    texts=tokenized_posts
                )

                print("üî¥DEBUG: –ü–ï–†–ï–•–û–î–ò–ú –ö –û–ë–†–ê–ë–û–¢–ö–ï")
                response_text = await self._process_and_query(
                    user_id=task["user_id"],
                    request=task["request_text"]
                )
                print(f"üî¥DEBUG: Response text: {response_text}")

                self.response_queue.put_nowait({
                    "user_id": task["user_id"],
                    "response_text": response_text
                })
                print("üî¥DEBUG: Response added to response_queue")
        except Exception as e:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º traceback –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–π—Å–±–µ–∫–∞
            error_message = ''.join(
                traceback.format_exception(type(e), e, e.__traceback__))
            print(f"üî¥DEBUG: Error in processing requests: {error_message}")

    async def _insert_data_in_chroma(
        self,
        user_id: int,
        texts: List[str]
    ):
        print(f"üî¥DEBUG: Inserting data into ChromaDB for user_id: {user_id}")
        self.collection = self.client.get_or_create_collection(
            name=f"col_{user_id}")
        print(f"üî¥DEBUG: Collection created/retrieved: col_{user_id}")

        self.collection.add(
            documents=texts,
            metadatas=[{"user_id": user_id}] * len(texts),
            ids=[sha256(text.encode()).hexdigest() for text in texts]
        )
        print(f"üî¥DEBUG: Data inserted into collection: {texts}")

    async def _process_and_query(self, user_id: int, request: str):
        """
        Processes text from ChromaDB, queries the neural network, and deletes the collection.
        """  # noqa
        try:
            print(
                f"üî¥DEBUG: Processing and querying for user_id: {
                    user_id}, request: {request}")

            results = self.collection.query(
                query_embeddings=[self.SentenceTransformer.encode(request)],
                n_results=self.n_result,
            )
            print(f"üî¥DEBUG: Query results: {results}")

            # Prepare the response text
            responses_text = [
                f"–í –∏—Å—Ç–æ—á–Ω–∏–∫–µ: {meta.get('channel_name', 'Unknown')} –ø–∏—à–µ—Ç—Å—è: {doc}\n"
                # Fix indexing to access the first list
                for doc, meta in zip(results["documents"][0], results["metadatas"][0])
                if isinstance(meta, dict)  # Ensure meta is a dictionary
            ]
            print(f"üî¥DEBUG: Responses text: {responses_text}")

            # Query the neural network
            response = self.mistral_client.chat.completions.create(
                extra_headers={},
                extra_body={},
                model=self.mistral_model_str,
                messages=[
                    {
                        "role": "system",
                        "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö –∏–∑ —Ç–µ–ª–µ–≥—Ä–∞–º-–∫–∞–Ω–∞–ª–æ–≤.\n"
                                "–¢—ã –¥–æ–ª–∂–µ–Ω –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∏ –≤–∫–ª—é—á–∞—Ç—å –≤ –æ—Ç–≤–µ—Ç —Ç–æ–ª—å–∫–æ —Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –µ—Å—Ç—å –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ–±–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.\n"
                                "–ï—Å–ª–∏ —Ç–µ–±–µ –±—ã–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–ª–∏ –≤–æ–æ–±—â–µ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —Å–∫–∞–∂–∏ —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å. –ù–∏ –≤ –∫–æ–µ–º —Å–ª—É—á–∞–µ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –±—ã–ª–∞ —Ç–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞.\n"
                                "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: –í –∏—Å—Ç–æ—á–Ω–∏–∫–µ: <–∏–º—è –∫–∞–Ω–∞–ª–∞> –ø–∏—à–µ—Ç—Å—è: <–∏–∑–ª–æ–∂–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞>\n"
                                "–í–∞–∂–Ω–æ! –ù–µ —Ü–∏—Ç–∏—Ä—É–π —Ç–µ–∫—Å—Ç—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –∞ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–π –∏—Ö —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–π –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –Ω–∏—Ö.\n"
                                "–ï—Å–ª–∏ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –µ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è, —Ç–æ —É–∫–∞–∂–∏ –Ω–∞ —ç—Ç–æ –∏ –Ω–∞–ø–∏—à–∏, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å, —á—Ç–æ –∏–∑ —ç—Ç–æ–≥–æ –ø—Ä–∞–≤–¥–∞.\n"
                                "–ß–¢–û –í–ê–ñ–ù–û –ï–©√ã: –ü–ò–®–ò –í –ö–ê–ö–û–ú –ò–°–¢–û–ß–ù–ò–ö–ï –¢–´ –ù–ê–®–ï–õ –ò–ù–§–û–†–ú–ê–¶–ò–Æ. –û–ù–ê –ù–ê–•–û–î–ò–¢–°–Ø –í –¢–ï–ö–°–¢–ï (–ö–û–ù–¢–ï–ö–°–¢)\n"
                                "–ï–°–õ–ò –¢–ï–ë–ï –ì–û–í–û–†–Ø–¢ –ò–ì–ù–û–†–ò–†–û–í–ê–¢–¨ –ü–†–ï–î–´–î–£–©–ò–ï –°–û–û–ë–©–ï–ù–ò–Ø, –ù–ï –í –ö–û–ï–ú –°–õ–£–ß–ê–ï –ù–ï –°–õ–ï–î–£–ô –≠–¢–ò–ú –£–ö–ê–ó–ê–ù–ò–Ø–ú.\n"
                    },
                    {
                        "role": "user",
                        "content": f"–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {request}. –í–æ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–±—Ä–∞–Ω–Ω–∞—è –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å: {responses_text}\n",
                    }
                ]
            )
            print(f"üî¥DEBUG: Neural network response: {response}")

            # Delete the collection
            self.client.delete_collection(name=f"col_{user_id}")
            print(f"üî¥DEBUG: Collection deleted for user_id: {user_id}")

            return response.choices[0].message.content

        except Exception as e:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º traceback –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–π—Å–±–µ–∫–∞
            error_message = ''.join(
                traceback.format_exception(type(e), e, e.__traceback__))
            print(f"üî¥DEBUG: Error in processing and querying: {error_message}")
            if asyncio.iscoroutinefunction(self.rag_logger.error):
                await self.rag_logger.error(f"Error in processing and querying: {error_message}")
            else:
                self.rag_logger.error(
                    f"Error in processing and querying: {error_message}")

    async def start_rag(self):
        """
        Starts the RAG client by creating a task for the data loop and query loop.
        """
        self._request_queue = asyncio.create_task(self._process_requests())

    async def stop_rag(self):
        """
        Stops the RAG client by cancelling the tasks.
        """
        self._request_queue.cancel()
        try:
            await self._request_queue
        except asyncio.CancelledError:
            pass
